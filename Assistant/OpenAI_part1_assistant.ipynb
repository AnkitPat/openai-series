{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NKXTHbJv3mo"
      },
      "source": [
        "# OpenAI Assistants APIs\n",
        "\n",
        "The Assistants' API lets you create AI assistants in your applications. These assistants follow and unnderstand instruction. They use models, tools, and knowledge to answer user questions. In this notebook we are going to use one of the tools, retriever, to query against pdf documents we will upload.\n",
        "\n",
        "The architecture and data flow diagram below depicts the interaction among all components that comprise OpenAI Assistant APIs. Central to understand is the Threads and Runtime that executes asynchronously, adding and reading messages to the Threads.\n",
        "\n",
        "For integrating the Assistants API in your application:\n",
        "\n",
        "1. Creat an Assistant with custom instructions and select a model. Optionally, enable tools like Code Interpreter, Retrieval, and Function Calling.\n",
        "\n",
        "2. Initiate a Thread for each user conversation.\n",
        "    \n",
        "3. Add user queries as Messages to the Thread.\n",
        "\n",
        "4.  Run the Assistant on the Thread for responses, which automatically utilizes the enabled tools\n",
        "5.  Await the Run to finish.\n",
        "\n",
        "Below we follow those steps to demonstrate how to integrate Assistants API, using Retrieval tool, to a) upload a couple of pdf documents and b) use Assistant to query the contents of the document. Consider this as a mini Retrieval Augmented Generation (RAG).\n",
        "\n",
        "The OpenAI documentation describes in details [how Assistants work](https://platform.openai.com/docs/assistants/how-it-works).\n",
        "\n",
        "<img src=\"./images/assistant_ai_tools_retriever.png\">\n",
        "\n",
        "**Note**: Much of the code and diagrams are inspired from  Randy Michak of [Empowerment AI](https://www.youtube.com/watch?v=yzNG3NnF0YE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qqwm_A4Ev3J9",
        "outputId": "0af82c94-37f1-498b-d47b-5162114dfc0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.28.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install python-dotenv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZHgK_XUv8Ir"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "Wk64x5JfwETi"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import os\n",
        "import time\n",
        "\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "from typing import List\n",
        "from assistant_utils import print_thread_messages, upload_files,\\\n",
        "                            loop_until_completed, create_assistant_run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJKC4o8ZwJg1"
      },
      "source": [
        "# Defining API keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6hYaoGswMf9",
        "outputId": "34ece0ef-4fe4-4e61-f6b8-8a3e46197190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using MODEL=gpt-4-1106-preview; base=https://api.openai.com/v1\n"
          ]
        }
      ],
      "source": [
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "\n",
        "openai.api_base = os.getenv(\"OPENAI_API_BASE\")\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "MODEL = os.getenv(\"MODEL\")\n",
        "print(f\"Using MODEL={MODEL}; base={openai.api_base}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyZYqR-awT-P"
      },
      "source": [
        "# Document to upload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "3Ibty3yAwVhu"
      },
      "outputs": [],
      "source": [
        "DOCS_TO_LOAD = [\"docs/HAI_AI-Index-Report_2023.pdf\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Madigz4pwYFO"
      },
      "source": [
        "# Initialising client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "s5H29UdswZ--"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(\n",
        "    api_key = openai.api_key,\n",
        "    base_url = openai.api_base,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOv1kk1KwctG"
      },
      "source": [
        "# Uploading file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7um2ntRQweRU",
        "outputId": "bd02008b-8cf5-4ff0-9ab5-562edd014e7d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[FileObject(id='file-fqkS0xOGFDDPwvFt0uhDWmIn', bytes=25318310, created_at=1715427039, filename='HAI_AI-Index-Report_2023.pdf', object='file', purpose='assistants', status='processed', status_details=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ],
      "source": [
        "file_objects = upload_files(client, files=DOCS_TO_LOAD)\n",
        "file_objects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tVcQAssw9gM"
      },
      "source": [
        "# Extract file ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E53MzkN0w_Di",
        "outputId": "6115bca6-c7e4-429b-a958-343d4a2e8909"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['file-fqkS0xOGFDDPwvFt0uhDWmIn']"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ],
      "source": [
        "# Extract file ids\n",
        "file_obj_ids = []\n",
        "for f_obj in file_objects:\n",
        "    file_obj_ids.append(file_objects[0].id)\n",
        "file_obj_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b89M_d_xCbI"
      },
      "source": [
        "### Step 2: Create an Assistant\n",
        "Before you can start interacting with the Assistant to carry out any tasks, you need an AI assistant object. Supply the Assistant with a model to use, tools, and file ids to use for its knowledge base."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3p-19b7xEnx",
        "outputId": "ae062b1b-6eda-4d0b-ce82-d26658c02473"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Assistant(id='asst_OoZZv2Ih5c1dTCzJ3YnDkRzf', created_at=1715427059, description=None, instructions=\"\\nYou are a knowledgeable chatbot trained to respond to inquiries on documents HAI Artificial Index 2023 report and Survey of why LLMs hallucinate.\\nUse a neutral, professional advisory tone, and only respond by consulting the knowledge base or files you are granted access to.\\nDo not make up answers. Consider your name as Ankit Patidar. If you don't know the answer, respond with 'Sorry, I'm afraid I don't have access to that information.'\\n\\n\", metadata={}, model='gpt-4-1106-preview', name='AI Report and LLM survey Chatbot', object='assistant', tools=[CodeInterpreterTool(type='code_interpreter')], response_format='auto', temperature=1.0, tool_resources=ToolResources(code_interpreter=ToolResourcesCodeInterpreter(file_ids=['file-fqkS0xOGFDDPwvFt0uhDWmIn']), file_search=None), top_p=1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ],
      "source": [
        "\n",
        "instructions = \"\"\"\n",
        "You are a knowledgeable chatbot trained to respond to inquiries on documents HAI Artificial Index 2023 report and Survey of why LLMs hallucinate.\n",
        "Use a neutral, professional advisory tone, and only respond by consulting the knowledge base or files you are granted access to.\n",
        "Do not make up answers. Consider your name as Ankit Patidar. If you don't know the answer, respond with 'Sorry, I'm afraid I don't have access to that information.'\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "assistant = client.beta.assistants.create(name=\"AI Report and LLM survey Chatbot\",\n",
        "                                           instructions=instructions,\n",
        "                                           model=MODEL,\n",
        "                                          tools=[{\"type\": \"code_interpreter\"}],\n",
        "  tool_resources={\n",
        "    \"code_interpreter\": {\n",
        "      \"file_ids\": file_obj_ids\n",
        "    }\n",
        "  }\n",
        "\n",
        ")\n",
        "assistant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDLE1lGI1T6Q"
      },
      "source": [
        "### Step 3: Create a thread\n",
        "As the diagram above shows, the Thread is the object with which the AI Assistant Runs will interact with, by fetching messages and putting messages to it. Think of a thread as a \"conversation session between an Assistant and a user. Threads store Messages and automatically handle truncation to fit content into a model’s context window.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYFRwwJ41WT4",
        "outputId": "a76f3a32-d2e0-4d50-d792-b8c781ae896b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Thread(id='thread_ouv0d3RE2nQe1ppJpeIKs2mX', created_at=1715427068, metadata={}, object='thread', tool_resources=ToolResources(code_interpreter=None, file_search=None))"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ],
      "source": [
        "thread = client.beta.threads.create()\n",
        "thread"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLr0VJhQ1ZEs"
      },
      "source": [
        "### Step 4: Add your message query to the thread for the Assistant\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ebdf6WuD1b8I",
        "outputId": "938bd312-6863-4293-87ff-56a4179a125e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'msg_DGMCqPeMPWYh3Vj8m3bPue2U',\n",
              " 'assistant_id': None,\n",
              " 'attachments': [],\n",
              " 'completed_at': None,\n",
              " 'content': [{'text': {'annotations': [],\n",
              "    'value': 'What are the top 10 takeaways in the Artificial Intelligence Index Report 2023.\\n    Summarize each takeway in no more three simple sentences.'},\n",
              "   'type': 'text'}],\n",
              " 'created_at': 1715427071,\n",
              " 'incomplete_at': None,\n",
              " 'incomplete_details': None,\n",
              " 'metadata': {},\n",
              " 'object': 'thread.message',\n",
              " 'role': 'user',\n",
              " 'run_id': None,\n",
              " 'status': None,\n",
              " 'thread_id': 'thread_ouv0d3RE2nQe1ppJpeIKs2mX'}"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ],
      "source": [
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=\"\"\"What are the top 10 takeaways in the Artificial Intelligence Index Report 2023.\n",
        "    Summarize each takeway in no more three simple sentences.\"\"\",\n",
        ")\n",
        "message.model_dump()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xj5Dbhw1gQ7"
      },
      "source": [
        "### Step 5: Create a Run for the Assistant\n",
        "A Run is an invocation of an Assistant on a Thread. The Assistant uses its configuration and the Thread’s Messages to perform tasks by calling models and tools. As part of a Run, the Assistant appends Messages to the Thread.\n",
        "\n",
        "Note that Assistance will run asychronously: the run has the following\n",
        "lifecycle and states: [*expired, completed, failed, cancelled*]. Run objects can have multiple statuses.\n",
        "\n",
        "<img src=\"https://cdn.openai.com/API/docs/images/diagram-1.png\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvRKV2_a1kSt",
        "outputId": "cf6e8157-704b-4169-9850-8f6fe4770b95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"id\": \"run_9U8xHgyOBbfVurNUaw4iwlbf\",\n",
            "    \"assistant_id\": \"asst_OoZZv2Ih5c1dTCzJ3YnDkRzf\",\n",
            "    \"cancelled_at\": null,\n",
            "    \"completed_at\": null,\n",
            "    \"created_at\": 1715427076,\n",
            "    \"expires_at\": 1715427676,\n",
            "    \"failed_at\": null,\n",
            "    \"incomplete_details\": null,\n",
            "    \"instructions\": \"Please address the user as Jules Dmatrix.\\n    Do not provide an answer to the question if the information was not retrieved from the knowledge base.\\n\",\n",
            "    \"last_error\": null,\n",
            "    \"max_completion_tokens\": null,\n",
            "    \"max_prompt_tokens\": null,\n",
            "    \"metadata\": {},\n",
            "    \"model\": \"gpt-4-1106-preview\",\n",
            "    \"object\": \"thread.run\",\n",
            "    \"required_action\": null,\n",
            "    \"response_format\": \"auto\",\n",
            "    \"started_at\": null,\n",
            "    \"status\": \"queued\",\n",
            "    \"thread_id\": \"thread_ouv0d3RE2nQe1ppJpeIKs2mX\",\n",
            "    \"tool_choice\": \"auto\",\n",
            "    \"tools\": [\n",
            "        {\n",
            "            \"type\": \"code_interpreter\"\n",
            "        }\n",
            "    ],\n",
            "    \"truncation_strategy\": {\n",
            "        \"type\": \"auto\",\n",
            "        \"last_messages\": null\n",
            "    },\n",
            "    \"usage\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"top_p\": 1.0,\n",
            "    \"tool_resources\": {}\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "instruction_msg = \"\"\"Please address the user as Jules Dmatrix.\n",
        "    Do not provide an answer to the question if the information was not retrieved from the knowledge base.\n",
        "\"\"\"\n",
        "run = create_assistant_run(client, assistant, thread, instruction_msg)\n",
        "print(run.model_dump_json(indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vN0RqxRy1neq"
      },
      "source": [
        "### Step 6: Loop through the Assistant run until status is 'completed'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yz_KI9UZ1pzu",
        "outputId": "58318636-9807-4afb-b4f1-1fd17ae56064"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"id\": \"run_9U8xHgyOBbfVurNUaw4iwlbf\",\n",
            "    \"assistant_id\": \"asst_OoZZv2Ih5c1dTCzJ3YnDkRzf\",\n",
            "    \"cancelled_at\": null,\n",
            "    \"completed_at\": 1715427079,\n",
            "    \"created_at\": 1715427076,\n",
            "    \"expires_at\": null,\n",
            "    \"failed_at\": null,\n",
            "    \"incomplete_details\": null,\n",
            "    \"instructions\": \"Please address the user as Jules Dmatrix.\\n    Do not provide an answer to the question if the information was not retrieved from the knowledge base.\\n\",\n",
            "    \"last_error\": null,\n",
            "    \"max_completion_tokens\": null,\n",
            "    \"max_prompt_tokens\": null,\n",
            "    \"metadata\": {},\n",
            "    \"model\": \"gpt-4-1106-preview\",\n",
            "    \"object\": \"thread.run\",\n",
            "    \"required_action\": null,\n",
            "    \"response_format\": \"auto\",\n",
            "    \"started_at\": 1715427076,\n",
            "    \"status\": \"completed\",\n",
            "    \"thread_id\": \"thread_ouv0d3RE2nQe1ppJpeIKs2mX\",\n",
            "    \"tool_choice\": \"auto\",\n",
            "    \"tools\": [\n",
            "        {\n",
            "            \"type\": \"code_interpreter\"\n",
            "        }\n",
            "    ],\n",
            "    \"truncation_strategy\": {\n",
            "        \"type\": \"auto\",\n",
            "        \"last_messages\": null\n",
            "    },\n",
            "    \"usage\": {\n",
            "        \"completion_tokens\": 69,\n",
            "        \"prompt_tokens\": 234,\n",
            "        \"total_tokens\": 303\n",
            "    },\n",
            "    \"temperature\": 1.0,\n",
            "    \"top_p\": 1.0,\n",
            "    \"tool_resources\": {}\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "run_status = client.beta.threads.runs.retrieve(\n",
        "    thread_id = thread.id,\n",
        "    run_id = run.id\n",
        ")\n",
        "print(run_status.model_dump_json(indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx9E2SF51vf7"
      },
      "source": [
        "#### Poll until Assistant run is completed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "F9NQvVCb1xkN"
      },
      "outputs": [],
      "source": [
        "loop_until_completed(client, thread, run_status)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qekNoulx11ny"
      },
      "source": [
        "### Step 7: Retrieve the message returned by the assistance\n",
        "Only when the run is **completed** can you fetch the messages from the Thread"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqIpbbhd14ML",
        "outputId": "44f997b7-3c62-4019-efd7-f488743a2e05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('assistant:To provide you with the top 10 takeaways from the Artificial '\n",
            " 'Intelligence Index Report 2023, I would need to access the content of the '\n",
            " \"report. If the file you've uploaded is the AI Index Report for 2023, I can \"\n",
            " 'extract the content and summarize the key points for you. Shall I proceed '\n",
            " 'with this file?')\n",
            "('user:What are the top 10 takeaways in the Artificial Intelligence Index '\n",
            " 'Report 2023.\\n'\n",
            " '    Summarize each takeway in no more three simple sentences.')\n"
          ]
        }
      ],
      "source": [
        "print_thread_messages(client, thread)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz9sewPA2BGm"
      },
      "source": [
        "### Repeat the process for any additional messages\n",
        "To add more query messages to the thread for the Assistant, repeat steps 5 - 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYTE3qjC2DPH"
      },
      "source": [
        "### Add another message to for the Assistant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNGhIEr22Fix",
        "outputId": "cd51e242-0bbb-4469-aead-701755d9aa2b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Message(id='msg_Ck5BW4oHcvoFpq6BBPIws4X9', assistant_id=None, attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='Yes please\\n    '), type='text')], created_at=1715427120, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='user', run_id=None, status=None, thread_id='thread_ouv0d3RE2nQe1ppJpeIKs2mX')"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ],
      "source": [
        "\n",
        "\n",
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=\"\"\"Yes please\n",
        "    \"\"\",\n",
        ")\n",
        "message"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMjRw8ct2IC0"
      },
      "source": [
        "### Create another run for the Assistant for the second message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txsywAM02KYs",
        "outputId": "5ec11065-3eab-46f6-83a9-84a1473fed41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "queued\n"
          ]
        }
      ],
      "source": [
        "run = create_assistant_run(client, assistant, thread, instruction_msg)\n",
        "run_status = client.beta.threads.runs.retrieve(\n",
        "    thread_id = thread.id,\n",
        "    run_id = run.id\n",
        ")\n",
        "\n",
        "print(run_status.status)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_gdsL_X2P4A",
        "outputId": "6425fabb-9483-422a-865f-5651c5028b9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in_progress\n",
            "in_progress\n",
            "completed\n"
          ]
        }
      ],
      "source": [
        "loop_until_completed(client, thread, run_status)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsCbCM3e2WZu"
      },
      "source": [
        "# Print"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCyeyUr22XP_",
        "outputId": "223dcbe7-4ec0-4bbc-d74f-657ff5f4a765"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('assistant:Based on the extracted text, here are some of the key takeaways '\n",
            " 'summarized from the Artificial Intelligence Index Report 2023:\\n'\n",
            " '\\n'\n",
            " '1. **Industry Dominance in ML Model Development**: Traditionally, academia '\n",
            " 'was the source of significant machine learning models. Since 2014, the '\n",
            " 'industry has ramped up its involvement, and in 2022, it produced 32 '\n",
            " 'significant ML models while academia produced only three. The shift is due '\n",
            " \"to the industry's greater resources like data, compute power, and funds.\\n\"\n",
            " '\\n'\n",
            " '2. **Performance Plateau on Traditional Benchmarks**: AI has continued to '\n",
            " 'achieve state-of-the-art results across various benchmarks. However, the '\n",
            " 'year-over-year improvement has been marginal, indicating a saturation in '\n",
            " 'performance on these traditional benchmarks. New benchmarks like BIG-bench '\n",
            " 'and HELM are being developed in response.\\n'\n",
            " '\\n'\n",
            " \"3. **AI's Environmental Dual Impact**: Research shows that AI systems have a \"\n",
            " 'significant environmental impact. While AI can help improve environmental '\n",
            " 'sustainability, it also poses harm due to the carbon footprint associated '\n",
            " 'with training and running large models.\\n'\n",
            " '\\n'\n",
            " 'The provided text seems to list these points as key takeaways. For '\n",
            " 'additional takeaways, I would need to process more of the document. Shall I '\n",
            " 'continue to extract the remaining takeaways from the report?')\n",
            "'user:Yes please\\n    '\n",
            "('assistant:To provide you with the top 10 takeaways from the Artificial '\n",
            " 'Intelligence Index Report 2023, I would need to access the content of the '\n",
            " \"report. If the file you've uploaded is the AI Index Report for 2023, I can \"\n",
            " 'extract the content and summarize the key points for you. Shall I proceed '\n",
            " 'with this file?')\n",
            "('user:What are the top 10 takeaways in the Artificial Intelligence Index '\n",
            " 'Report 2023.\\n'\n",
            " '    Summarize each takeway in no more three simple sentences.')\n"
          ]
        }
      ],
      "source": [
        "print_thread_messages(client, thread)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}